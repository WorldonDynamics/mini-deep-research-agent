<!-- Copilot instructions for mini-deep-research-agent -->
# Copilot / AI contributor instructions

These notes help an AI coding agent to be immediately productive in this repository.

**Repository Focus:**: This project loads research paper datasets, computes sentence-transformer embeddings, performs semantic search, and generates short summaries.

- **Entry points:**: `src/main.py` (demo runner) and `mini-deep-research-agent/src/main.py` (template runner). Run with `python src/main.py` from the repository root.
- **Core utilities:**: `src/utils.py` (embedding, caching, semantic search, summarization). This file holds the important patterns to follow.

**Important Files & Paths:**
- **Data:**: `Data/raw/dummy_papers.csv` (source CSV). Processed artifacts (caches) live under `Data/processed/`.
- **Embeddings cache:**: `Data/processed/embeddings.pkl` (generated by `embed_texts`). Use the existing cache when present to avoid re-encoding.
- **Output:**: scripts save summaries to `output/multi_query_results.json` (script creates directories as needed).

**Architecture & Patterns (what to preserve):**
- **Single global model instance:**: `SentenceTransformer('all-MiniLM-L6-v2')` is created at import time in `utils.py` to avoid repeated loads. Avoid changing this to per-call instantiation unless you add explicit lazy-loading.
- **Relative data paths:**: Code uses relative paths (e.g., `../Data/raw/dummy_papers.csv`) and a `BASE_DIR` resolution in some modules. When moving files or refactoring, preserve how paths are constructed to keep scripts runnable from the repository root.
- **Cache-first embedding flow:**: `embed_texts` checks `os.path.exists(cache_file)` and returns cached embeddings. Any rework must keep or intentionally replace this cache behavior and update tests or README accordingly.

**Developer workflows & commands**
- **Install deps (PowerShell):**: `pip install -r requirements.txt`
- **Run demo:**: from `mini-deep-research-agent` root run `python src/main.py` (or `python -m src.main`). This will read `Data/raw/dummy_papers.csv`, build/load embeddings, run queries, and write `output/multi_query_results.json`.
- **Regenerate embeddings manually:**: Remove `Data/processed/embeddings.pkl` then run the main script to force re-encoding.

**Project-specific guidance for changes**
- **When changing embeddings model:**: Update `model = SentenceTransformer(...)` in `src/utils.py` and ensure `requirements.txt` is updated if switching frameworks (e.g., add `torch` if switching to a model that requires it). Clear or version the cache file so old embeddings are not misused.
- **When changing Data schema:**: `semantic_search` concatenates `df['title'] + ". " + df['abstract']`. If adding/removing columns, update this composition in `semantic_search` and any code that assumes `title`/`abstract`/`authors` exist (e.g., `summarize_papers`).
- **Avoid heavy imports at test time:**: Because the model loads on import, helpers that will be unit-tested should either mock the model or add a light import guard (e.g., lazy_load_model) rather than importing heavy models at module top-level.

**Examples (use these in fixes and tests)**
- To run the main workflow from the repository root (PowerShell):

```
pip install -r requirements.txt
python src/main.py
```

- To force embeddings refresh:

```
Remove `Data/processed/embeddings.pkl` then run `python src/main.py`.
```

**Where to look for related code**
- `mini-deep-research-agent/src/utils.py` — embedding, cache patterns, semantic search, summarization.
- `mini-deep-research-agent/src/main.py` — example queries, output save behavior.
- `Data/raw/dummy_papers.csv` — sample data fields and shapes to mirror in tests/mocks.

If anything here is unclear or you'd like more detail (for example, explicit tests to add or a suggested lazy-loading pattern), tell me which section to expand and I'll update this file.
